role :          null
world_size:     null
device_num:     null
local_rank:     null
do_grpo:        True
sft_or_grpo_rm: True # False. the initial run used False (i.e. GPT4o)
use_own_loader: True
bsz:            1 # used by ranker only
test_print:     False
use_optimum:    False
dirpath:        /workspace/agentic_qud
hub_dirpath:    /workspace/llm_models/hub
savepath:       null
hf_token:       null
token_dict:     null
load_ckpt_path: null
load_peft_ckpt_path:          null
merge_peft_weights:           False
exclude_used_q_i_ids:         True
exclude_used_q_i_ids_files:   "" # separated by ;
train_exclude_long_prompts:   False
use_accelerate:               False
use_vllm:                     False
shorten_filename:             False
flash_attn_override:          null
rm_device_override:           null
do_eval_only_bypass:          False   # i.e. run inference only; no training
pad_token:      null
device:         null
exp_code:       null
add_tokens:             []
savepath_train_outputs: null
savepath_test_outputs:  null
exclude_same_scores:    True
save_steps:             250
warmup_ratio:           0.1
eval_every_save_step:   True
models_force_4bit:      ['google/gemma-2-27b-it']
models_force_8bit:      []
criteria_reward_funcs:  ['criteria2_reward_func', 
                        'criteria3_reward_func', 
                        'criteria4_reward_func']
reward_funcs_version:   1
send_end:       s_end
qud_sep:        '[Q]'
score_sep:      '[S]'
qud_criteria_list: [criteria2, criteria3, criteria4]
#######################################################
architecture:
  master:
    addr: null
    port: '29500'
    rank: 0 
  workers: 
    criteria2: 
      port: '29500'
      rank: 1
    criteria3:
      port: '29500'
      rank: 2
    criteria4:
      port: '29500'
      rank: 3
#######################################################

grpo_settings:
  grpo_task:                    qud_gen # or 'rankllm'
  max_steps:                    2000    # set this considering gradient_accumulation_steps
  skip_idxes:                   null
  initial_rules_based_steps:    1e9     # 1e9 is to only use rules-based
  beta:                         0.04
  epsilon:                      0.2     # default value in TRL, also value in Schulman et al 2017 (https://arxiv.org/pdf/1707.06347). bounds the size of the policy update 
  epsilon_high:                 0.2     # DAPO finding
  reward_weights:               null
  sync_ref_model:               False
  ref_model_mixup_alpha:        0.9
  ref_model_sync_steps:         64
  # this should Nx of num_cands, i.e. N/num_cands is the number of prompts per batch
  gen_bsz:                      4       # this controls per_device_train_batch_size in TrainerArgs
  num_cands:                    4       # this controls how many candidates per prompt
  gradient_accumulation_steps:  4       # impacts size of global_step 
  num_iterations:               1       
  swop_rules_llmqalogprobs:     True 
  peft_target_modules:          null    # this will be set in main .py
  use_dora:                     False
  lora_rank:                    32
  lora_alpha_ratio:             0.5
  lora_dropout:                 0.05
  max_seq_length:               512     # this is thinking tokens + answer (QUD)
  max_prompt_length:            1200
  gpu_memory_utilization:       0.6
  # temperature to sample num_cands from each prompt
  # the more different, the better for exploration
  temperature:                  1.0
  scale_rewards:                False

###########
model:
  sft:
    model:    qwen # llama
    size:     mini
    load_in_nbit: False # False, 8 or 4
  reward:
    use_past_key_values:  True
    # RM could be (gpt4o, closed_models)   -- initial model (where GPT4o ranking+scores knowledge was obtained)
    # or also (qwen/llama, sft_rm/grpo_rm) -- 2nd tranches of models
    criteria2:
      model:    gpt4o 
      size:     closed_models # if gpt4o
    criteria3:
      model:    gpt4o
      size:     closed_models
    criteria4:
      model:    gpt4o
      size:     closed_models
  models_list:
    mini:
      phi:        microsoft/Phi-3.5-mini-instruct
      llama:      meta-llama/Llama-3.2-3B-Instruct
      gemma:      google/gemma-2-2b-it
      qwen:       Qwen/Qwen2.5-3B-Instruct
    minipt: # pretrained only, i.e. non-chat, RLHF
      llama:      meta-llama/Llama-3.2-3B
      gemma:      google/gemma-2-2b
      qwen:       Qwen/Qwen2.5-3B
    small:
      phi:        microsoft/phi-4
      llama:      meta-llama/meta-Llama-3.1-8B-Instruct
      gemma:      google/gemma-2-9b-it
      qwen:       Qwen/Qwen2.5-7B-Instruct
      dsr1_llama: deepseek-ai/DeepSeek-R1-Distill-Llama-8B
      dsr1_qwen:  deepseek-ai/DeepSeek-R1-Distill-Qwen-7B
    smallpt:
      llama:      meta-llama/Llama-3.1-8B
      gemma:      google/gemma-2-9b
      qwen:       Qwen/Qwen2.5-7B
    unsloth_large: 
      # phi: loading in bfloat16 (model is only 14B)
      phi:        microsoft/phi-4 
      llama:      unsloth/Llama-3.3-70B-Instruct-bnb-4bit
      gemma:      google/gemma-2-27b-it
      qwen:       unsloth/Qwen2.5-72B-Instruct-bnb-4bit
      dsr1_llama: unsloth/DeepSeek-R1-Distill-Llama-70B-bnb-4bit
      dsr1_qwen:  unsloth/DeepSeek-R1-Distill-Qwen-32B-bnb-4bit
    closed_models: 
      gpt4o:      gpt-4o-2024-08-06
      o3mini:     o3-mini-2025-01-31
    asst_stop_sequences: {"gemma": "<end_of_turn>", "llama": "<|eot_id|>", "phi": "<|end|>", "qwen":"<|im_end|>"}

##### GENERATION #####
rerank_constrained_bs: 
  do_constrained:   False
  num_beams:        null

gen_args:
  do_sample:    False
  top_p:        null
  top_k:        null
  # temperature:  0.0
  max_new_tokens: null
  min_new_tokens: null
  constraints:  null
  num_beams:    null
  pad_token_id: null
  eos_token_id: null
  rankllm: 
    max_new_tokens: 20
  # qud_gen: 
  #   max_new_tokens: 1024
  forced_bos_token:
    rankllm: '<think>'
    qud_gen: '<think>'
  # words to ensure excluded from span in answer tags
  # NOTE: ensure prefix space is added
  tighter_qud_gen_phrases: [' according to the anchor', 'according to the answer', ' according to the context', 
                        ' in the context', ' in the anchor', ' in the answer', 'in the speech', 
                        ' of the context', ' of the anchor', ' of the answer', 'of the speech', 
                        ' according to the Anchor', 'according to the Answer', ' according to the Context', 
                        ' in the Context', ' in the Anchor', ' in the Answer', 'in the Speech', 
                        ' of the Context', ' of the Anchor', ' of the Answer', 'of the Speech', 
                        ' the context', ' the anchor', ' the answer', 'the speech',
                        ' the Context', ' the Anchor', ' the Answer', 'the Speech',
                        ' CTX', ' ANS', ' ANC', ]
  tighter_qud_gen_phrases_reward: [' the context', ' the anchor', ' the answer', ' CTX', ' ANS', ' ANC', ]

ranker_args:
  # context_size used in rankllm context size. sets maxlength of tokenizer. 
  # could get stuck in self.get_num_tokens if too low for CoT (rec: 8192)
  # phase1_use_closed_models: True
  # NOTE: all criteria has to have all keys be True for cot output with scores
  # NOTE: add_task_decomp_cot should be set ot False if using SFT/GRPO RM (instead of GPT4o, i.e. initial model)
  rm_criteria_settings: {"criteria2": {"do_cot": True, "cot_json": True, "cot_fine": True, 
                                       "add_task_decomp_common": True, "add_task_decomp_cot": True}, 
                         "criteria3": {"do_cot": True, "cot_json": True, "cot_fine": True, 
                                       "add_task_decomp_common": True, "add_task_decomp_cot": True}, 
                         "criteria4": {"do_cot": True, "cot_json": True, "cot_fine": True, 
                                       "add_task_decomp_common": True, "add_task_decomp_cot": True}, }
  context_size:           8192 
  num_few_shot_examples:  3
  add_task_decomp_common: True
  add_task_decomp_cot:    False
  do_cot:                 True
  cot_json:               True
  cot_fine:               True
  reasoning_instruct:     False
  reasoning_rep_key:      'nline' # nline_guided if training/using SFT/GRPO RM model. used in rank_listwise_os_llm.py 
  use_past_key_values:    False
  num_gpus:               1
  variable_passages:      True
  window_size:            4
  step_size:              null
  top_k_candidates:       null
  # system_message:         null
  bypass_fsc_load:        True
  rerank_task_name:       'qud'
  device:                 null
  rank_pred_seq:          null
  rank_pred_seq_tokens:   null
  rank_pred_seq_tokens_dec: null
  shuffle_candidates:     False
  print_prompts_responses: False
  qud_min_score:          1.0
  qud_max_score:          3.0
  reward_model_trg_data:  [
    'grpo_settings-ms-500-si-N-irbs-0-b-0.04-e-0.2-rw-N-srm-F-rmma-0.9-rmss-64-qgb-4-nc-4-gas-4-ni-1-srl-T-ptm-N-ud-F-lr-32-lar-0.5-ld-0.05-msl-512-mpl-1200-gmu-0.6-t-1.0_models-llama-mini-C2-gp-C3-gp-C4-gpQUD_GEN-RANK-COMTDECOMP_COT-3-FINE-JSON_LPC-0',
    'grpo_settings-ms-500-si-N-irbs-0-b-0.04-e-0.2-rw-N-srm-F-rmma-0.9-rmss-64-qgb-4-nc-4-gas-4-ni-1-srl-T-ptm-N-ud-F-lr-32-lar-0.5-ld-0.05-msl-512-mpl-1200-gmu-0.6-t-1.0_models-llama-mini-C2-gp-C3-gp-C4-gpQUD_GEN-RANK-COMTDECOMP_COT-3-FINE-JSON_LPC-1',
    'grpo_settings-ms-500-si-N-irbs-0-b-0.04-e-0.2-rw-N-srm-F-rmma-0.9-rmss-64-qgb-4-nc-4-gas-4-ni-1-srl-T-ptm-N-ud-F-lr-32-lar-0.5-ld-0.05-msl-512-mpl-1200-gmu-0.6-t-1.0_models-llama-mini-C2-gp-C3-gp-C4-gpQUD_GEN-RANK-COMTDECOMP_COT-3-FINE-JSON_LPC-2',
  ]

qud_gen:
  do_icl :                False # True
  num_few_shot_examples:  0 # 3
  add_task_decomp_common: False
  add_task_decomp_cot:    False
  add_terminology:        False
  add_criteria_desc:      True
  add_term_desc_loc:      system
  do_cot:                 null
  cot_json:               null
  cot_fine:               null
  do_reasoning:           True
  use_past_key_values:    False

answer_compat: 
  do_icl :                True
  num_few_shot_examples:  2
  model_names:            ['llama']
  model_sizes:            ['minipt']
  grpo_model_name:        'llama'
  grpo_model_size:        'minipt'
  ans_cands_form:         'all' 
  # or "post_anc". "post_anc" gives rise to superfluous cases where there is only 1 remaining sentence 
  add_index_nums:         True

##### EXEMPLARS & PROMPTS #####
prompts:
  answer_compat: 
    system_message: 'You are an intelligent assistant that can carefully analyse an article and a question that is related to it in order to answer the question.'
    instructions: 'I will give you a part of an article, followed by a question. I will then provide you with a number of potential candidates that will answer the question. Your task is to select the most appropriate answer candidate based on the contents of the article and the question. It is very important that you reply by repeating exactly (i.e. word-for-word) the candidate you select as the answer. Start your reply by giving the answer immediately, do not say anything else.\n'
    prefix: |
      {{instructions}}

      ## Article: 
      {{context}}

      ## Question:
      {{qud}}

      ## Answer Candidates:
      {{ans_cands}}

      ## Answer:
    icl_exemplars: null
  rankllm:
    system_message: 'You are an intelligent assistant that is as well-trained as a graduate linguistics student. You have the capabilities to understand a set of linguistic annotation instructions and can rank the quality of a set of Questions Under Discussion (QUD) instances for some specified Criteria and Scoring Scheme.'
    # see https://github.com/huggingface/open-r1/blob/138df0ca44d5799fb24db2b50ff8f029c37aeca0/src/open_r1/grpo.py#L105
    system_message_reasoning: 'A conversation between User and Assistant. The User describes a task and the Assistant solves it. The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer. The reasoning process is enclosed within <think> </think> tags and the answer is enclosed within <answer> </answer> tags, i.e. <think> {reasoning process here} </think> and <answer> {answer here} </answer>. The Assistant is as well-trained as a graduate linguistics student and has the capabilities to understand a set of linguistic annotation instructions, therefore the Assistant is able to rank the quality of a set of Questions Under Discussion (QUD) instances for some specified Criteria and Scoring Scheme.'
    prefix:
      context_empty: '[EMPTY]'
      common: |
        I will provide you with {{num_cands}} attempts that were made at generating a Questions Under Discussion (QUD) instance. Each attempt is indicated by a numerical identifier [] in front of it. Your task is to rank the attempts based on their quality with respect to the Criteria (using the Scoring Scheme) specified below. 
        
        {{terminology}}
        Here is the criteria that you should judge the generated QUDs for:
      terminology: |
        ###########
        First of all, here are some important terminology and definitions to help you to understand the task:
        
        ## Terminology and definitions:
        - "Questions Under Discussion":   this is a linguistic framework for representing discourse structure -- a piece of discourse (for e.g. a news article) can be seen as a sequence of (implicit) questions that are successively posed and answered as the discourse progresses.
        - "QUD":                          a QUD is an (implicit) question that links two parts in a piece of discourse. In practice, a QUD is generated given the following inputs that are drawn from the piece of discourse: (i) the context; (ii) an anchor sentence, and (iii) an answer sentence. To be useful in a QUD-based representation of the piece of discourse, the QUD has to be carefully crafted to meet certain linguistic constraints with respect to the context, anchor sentence and/or answer sentence (depending on which Criteria is being assessed).
        - "context (CTX)":                this is the established ground (i.e. information that has already been encountered in the discourse up to, but not including, the anchor sentence). The CTX can be empty (marked by a "{{context_empty}}" symbol) if the anchor sentence is the first sentence in the discourse.
        - "anchor sentence (ANC)":        this is a sentence in the discourse that directly evokes (i.e. anchors) the QUD. The QUD should be one that a person reading up to the point of the ANC in the discourse is likely to have come to mind (i.e. the next thing the person would be curious to read is the answer to the QUD).
        - "answer sentence (ANS)":        this is a sentence further down in the discourse from the CTX and ANC, and whose Focus (i.e. its main part) answers the QUD. The ANS (i) is a different sentence from that of the ANC, and (ii) never comes before the ANC.
        - "Focus":                        this is the main part of a sentence (in our task here, either the ANC or the ANS). i.e. it is the most important new information that the speaker/writer wants to communicate to their audience. It is distinct from other background information in the sentence sentence. It is also termed as "at-issue" content. 
        - "Discourse-Old":                this describes concepts (entities, events, or states) that have already been mentioned in the CTX or ANC.
        - "Mediated":                     this describes concepts that have not been directly mentioned in the CTX or ANC, but are generally known or can be inferrable from one of the concepts that has already been mentioned (i.e. "Discourse-Old").
        
        ###########
      criteria2: |
        {{common}}
        
        ## Criteria:                Answer Compatibility
        ## Definition of Criteria:  this assesses whether the QUD is appropriately answered by the answer sentence (ANS). The reason this Criteria matters is because, for a QUD to be useful, it has to be fully resolvable by the most important information of the ANS, which makes it a good link between the ANC and ANS.
        ## Scoring Scheme:
        - "direct and explicit":    the Focus (and only the Focus) of the ANS answers the QUD. (3 points)
        - "unfocused":              the ANS contains the answer to the QUD; however, the answer is not the ANS's Focus. (2 points)
        - "not answered":           the ANS does not answer the QUD. (1 point)
        {{task_decomposition}}
      criteria3: |
        {{common}}
        
        ## Criteria:                  Givenness
        ## Definition of Criteria:    this assesses whether the QUD only contains concepts that (i) are in the context (CTX) or the anchor sentence (ANC) (i.e. "Discourse-Old"), or else (ii) the concepts are generally known, or can be easily inferred from concepts in the CTX and/or the ANC (i.e. "Mediated"). This Criteria matters because, for a QUD to be useful, it should not leak all or part of the answer, nor should it hallucinate information. 
        ## Scoring Scheme:
        - "no new concepts":          all of the concept(s) mentioned in the QUD is/are either "Discourse-Old" or "Mediated". (3 points)
        - "answer-leakage":           the QUD contains concepts that are neither "Discourse-Old" nor "Mediated". However, these new concepts can be found in the ANS, which is what distinguishes it from a case of "hallucination". (2 points)
        - "hallucination":            the QUD contains new concepts that are neither "Discourse-Old" nor "Mediated". Furthermore, these new concepts also cannot be found in the ANS, which is what distinguishes it from a case of "answer-leakage". (1 point)
        {{task_decomposition}}
      criteria4: |
        {{common}}

        ## Criteria:                  Anchor Relevance
        ## Definition of Criteria:    this assesses whether the QUD only/mostly contains information found in the ANC. The general idea of this Criteria is to establish whether a person could raise the QUD having just read the ANC (in other words, it is the most salient information up to this point of the discourse -- i.e. from the ANC -- that should be the trigger of a person's curiosity for the additional information to be sought, and the QUD should be formed to reflect this). 
        ## Scoring Scheme:
        - "fully grounded":           all/most of the content in the QUD follows from the ANC. (3 points)
        - "partially grounded":       only some content in the QUD is grounded in the ANC. (2 points)
        - "not grounded":             most/all of the content in the QUD is not from the ANC. (1 point)
        {{task_decomposition}}
    post: 
      cot_start:  '[S_COT]'
      cot_end:    '[E_COT]'
      common: | 
        
        Rank the {{num_cands}} attempts at QUD generation above. Rank them based on their quality with respect to the Criteria and the Scoring Scheme specified. All the QUD attempts should be included and listed using identifiers and in descending order of quality. If there are several attempts that are equally good amongst themselves, order their ranking by their index number (smallest first). The output format should be [] > [], e.g., {{example_ordering}}. Only respond with the ranking results and stop immediately; do not say anything else.
      common_reasoning_replace: {'oline': 'All the QUD attempts should be included and listed using identifiers and in descending order of quality. If there are several attempts that are equally good amongst themselves, order their ranking by their index number (smallest first). The output format should be [] > [], e.g., {{example_ordering}}. Only respond with the ranking results and stop immediately; do not say anything else.', 
                                 'nline': "Start by thinking (be sure to use the <think> </think> tags), then answer with the ranking results (be sure to use the <answer> </answer> tags). All the QUD attempts should be included and listed using identifiers and in descending order of quality. If there are several attempts that are equally good amongst themselves, order their ranking by their index number (smallest first). The output format should be [] > [], e.g., {{example_ordering}}. Once you have responded with the ranking results, stop immediately; do not say anything else.", 
                                 'nline_guided': "Start by thinking (be sure to use the <think> </think> tags), then answer with the ranking results (be sure to use the <answer> </answer> tags). Structure your thoughts in JSON format. There should be one JSON-parseable object for every QUD attempt (i.e. candidate). Each JSON object has to be of this form: {\"candidate\": ..., \"rationale\": ..., \"score\": ...}. Note that the scores must range from {{qud_min_score}} to {{qud_max_score}}. All of the JSON objects should be enclosed by these two tags: {{cot_start}} and {{cot_end}}. In your answer, all of the QUD attempts should be: (i) included, (ii) listed using identifiers, and (iii) in descending order of quality. If there are several attempts that are equally good amongst themselves, order their ranking by their identifier number (smallest first). The output format of your answer should be [] > [], e.g., {{example_ordering}}. Once you have responded with the ranking results, stop immediately; do not say anything else."
                                 }
      common_cot_replace:       {'oline': 'Only respond with the ranking results and stop immediately; do not say anything else.', 
                                 'nline': "It is very important that you return the ranking results surrounded with '[START]' and '[STOP]'. Once you have responded with the ranking results, stop immediately; do not say anything else."}
    task_decomposition: 
      criteria2: |
        ## Guide to Scoring Scheme:
        When examining each QUD Attempt, the first thing you need to do is establish whether the QUD is answered by the ANS. You must be very careful and thorough doing this; be very precise and check whether the ANS properly answers the QUD. There are two possibilities at this point -- either yes or no. If it is not, then you can immediately conclude that the QUD is a case of "not answered". If it is, then you need to go on to another step to establish if the part of the ANS that answers the QUD is the Focus of the ANS. There are again two possibilities at this point -- either yes or no. If it is not (i.e. the answer to the QUD is not the Focus of the ANS), then it is a case of "unfocused"; if it is, then it is a case of "direct and explicit".  

      criteria3: |
        ## Guide to Scoring Scheme:
        When examining each QUD Attempt, the first thing you need to do is extract the set of concept(s) that are mentioned, or referred to, in (i) the QUD, and (ii) the CTX together with the ANC. Here, you have to be very careful to take into consideration referring expressions and count them as mentions too. You should also be careful not to consider what is being asked for in the QUD as one of the concepts. The next step is to compare the sets from (i) and (ii); you will need to decide if the concept(s) mentioned in the QUD -- i.e. set (i) -- are Discourse-Old or Mediated; meaning that all of the concept(s) in the QUD: (a) are either mentioned in the CTX and/or ANC, or (b) they can be easily inferred from the concept(s) already mentioned, or (c) they are generally known information. There are two possibilities at this point. The first is that set (i) from the QUD is a complete subset of set (ii) --- i.e. all of the concept(s) in the QUD are Discourse-Old or Mediated, and so you can immediately conclude that it is a case of "no new concepts". The second is that set (i) has concepts that are not Discourse-Old or Mediated. If so, then you need to go to the next step and check if the concept(s) that is/are not Discourse-Old or Mediated can be found in the ANS. There are again two possibilities at this point -- either yes or no. If the concept(s) that are not Discourse-Old or Mediated can be found in the ANS, then it is a case of "answer-leakage". If the concept(s) cannot be found in the ANS, then it is a case of "hallucination". 

      criteria4: |
        ## Guide to Scoring Scheme:
        When examining each QUD Attempt, the first thing you need to do is extract the key pieces of information that is mentioned, or referred to, in (i) the QUD, and (ii) the ANC. Here, you have to be very careful to take into consideration referring expressions and count them as mentions too. You should also be careful not to consider what is being asked for as one of the key pieces of information. The next step is to compare the sets from (i) and (ii); you will need to decide if all/most, some or none of the key pieces of information mentioned in the QUD -- set (i) -- are found in set (ii).  There are three possibilities at this point. The first possibility is that set (i) from the QUD is a complete or near complete subset of set (ii), i.e. all or most of content of the QUD follows from the ANC and therefore this is a case of "fully grounded". The second possibility is that only half or less of the information in set (i) is found in set (ii), then this is a case of "partially grounded". The third possibility is that only a small amount, or none, of the information from set (i) can be found in set (ii), then this a case of "not grounded". 

    icl_exemplars: null

  qud_gen: 
    system_message: You are an intelligent assistant that is as well-trained as a graduate linguistics student. You have the capabilities to understand a set of linguistic annotation instructions and can write a question that meets the requirements of the Questions Under Discussion (QUD) framework. The framework specifies a certain set of Criteria the questions must meet.
    system_message_reasoning: A conversation between User and Assistant. The User describes a task and the Assistant solves it. The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer. The reasoning process is enclosed within <think> </think> tags and the answer is enclosed within <answer> </answer> tags, (i.e. <think> {reasoning process here} </think> and <answer> {answer here} </answer>). The Assistant is as well-trained as a graduate linguistics student and has the capabilities to understand a set of linguistic annotation instructions, therefore the Assistant can write a question that meets the requirements of the Questions Under Discussion (QUD) framework. The QUD framework specifies a certain set of Criteria that the question must meet.
    prefix: 
      context_empty: '[EMPTY]'
      common: |
        I will provide you with information taken from a piece of discourse: (i) the CTX, (ii) the ANC, and (iii) the ANS. Your task is to write a question that is a QUD linking ANC and ANS, and which meets all of the Criteria listed below.
                
        {{terminology}}
    post: 
      common: Write a QUD given the CTX, ANC and ANS above. Make sure that the QUD you write scores maximum points on all of the Criteria specified. Only respond with the QUD and stop immediately; do not say anything else.
      common_reason: Write a QUD given the CTX, ANC and ANS above. Make sure that the QUD you write scores maximum points on all of the Criteria specified. Start by thinking (it is very important to use the <think> </think> tags), then give the QUD in your answer (it is also very important to use the <answer> </answer> tags). Do not say anything else after your answer.
      common_cot_replace:       {'oline': 'Only respond with the QUD and stop immediately; do not say anything else.', 
                                  'nline': "It is very important that you return the QUD surrounded with '[START]' and '[STOP]'. Once you have responded with the QUD, stop immediately; do not say anything else."}
    task_decomposition: 'Step 1: you need to extract the set of concept(s) that are mentioned, or referred to, in (i) the QUD, (ii) the ANS, and (iii) the CTX together with the ANC.  Here, you have to be very careful to take into consideration referring expressions and count them as mentions too. \nStep 2: you should form a QUD that will be answered by the ANS. \nStep 3: you must check that the QUD meets the Answer Compatibility Criteria (i.e. that the QUD is answered by the Focus - and only the Focus - of the ANS). \nStep 4: you must then check that the QUD meets the Givenness Criteria (i.e. that the QUD only contains concepts that are Discouse-Old or Mediated; which means that the the concepts in the QUDs are all mentioned in the CTX and/or the ANC, or else can be easily inferred from those there or are generally known). \nStep 5: you must check that the QUD meets the Anchor Relevance Criteria (i.e. the QUD only contains information that is found in the ANC). If the initial QUD does not meet any of these requirements, refine or rewrite the QUD to make it meet all of the Criteria.'
